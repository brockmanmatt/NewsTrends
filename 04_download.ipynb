{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions for how to build this using nbdev at https://nbdev.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and format datasets of news articles\n",
    "\n",
    "> Takes a loader as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets to get:\n",
    "-CoverageTrends\n",
    "\n",
    "-GDELT\n",
    "\n",
    "-Are there others that are archived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download CoverageTrends articles\n",
    "\n",
    "CoverageTrends is a git repo I put together a few weeks ago to start playing with this. It creates daily CSVs for different publishers, updated every 30 minutes with front page coverage. The articles are in CSV form at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'github'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-7c855228f3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgithub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGithub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'github'"
     ]
    }
   ],
   "source": [
    "from github import Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download GDELT Headlines; not sure there's a memory efficient way to do this. Possibly doing some sort of integration with BigQuery?\n",
    "from https://blog.gdeltproject.org/announcing-gdelt-global-frontpage-graph-gfg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GDELT files are pretty large (250mb/hr zipped, 1.5gb unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def downloadGDELT(filepath, year:\"YYYY\", month:\"MM\", day: \"DD\", hour:\"HH\"):\n",
    "    \"download and extract .gz, borrowed from https://stackoverflow.com/questions/3548495/download-extract-and-read-a-gzip-file-in-python\"\n",
    "    \n",
    "    \"\"\"\n",
    "    WARNING: This results in a 1.5gb file per hour, or 30gb/day. Not actually sure best way to go about making this useful.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"http://data.gdeltproject.org/gdeltv3/gfg/alpha/{}{}{}{}0000.LINKS.TXT.gz\".format(year, month, day, hour)\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    out_file = \"{}/{}\".format(filepath, url.split(\"/\")[-1][:-3])\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            with gzip.GzipFile(fileobj=response) as uncompressed:\n",
    "                file_content = uncompressed.read()\n",
    "\n",
    "            with open(out_file, 'wb') as f:\n",
    "                f.write(file_content)\n",
    "                return file_content\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = downloadGDELT(\"test\", \"2020\", \"06\", \"08\", \"00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200608000000.LINKS.TXT']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test/20200608000000.LINKS.TXT\", sep='\\\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available = test[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nytimes.com/']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in available if x.find(\"nytimes\") > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.wsj.com/',\n",
       " 'https://www.actionnewsjax.com/',\n",
       " 'http://wsjy.hebnews.cn/',\n",
       " 'https://www.wnewsj.com/',\n",
       " 'https://diabetesnewsjournal.com/',\n",
       " 'https://www.ctnewsjunkie.com/',\n",
       " 'https://www.mansfieldnewsjournal.com/',\n",
       " 'http://newsjunkiepost.com/',\n",
       " 'https://gulfnewsjournal.com/',\n",
       " 'https://www.wsjm.com/',\n",
       " 'https://www.thenewsjournal.net/',\n",
       " 'https://www.benewsjournal.com/',\n",
       " 'https://stocknewsjournal.com/']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in available if x.find(\"wsj\") > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnnturk.com/',\n",
       " 'https://cnnphilippines.com:443/',\n",
       " 'http://ncnn.com/',\n",
       " 'https://arabic.cnn.com/',\n",
       " 'https://cnnespanol.cnn.com/?redirect=cnnmexico',\n",
       " 'https://www.cnnbs.nl/',\n",
       " 'http://www.cnn21.co.kr/',\n",
       " 'https://www.cnnindonesia.com/',\n",
       " 'https://www.cnnchile.com/',\n",
       " 'https://cnnespanol.cnn.com/',\n",
       " 'https://www.cnn.gr/',\n",
       " 'http://www.acnnewswire.com/',\n",
       " 'http://www.cnnb.com.cn/',\n",
       " 'https://www.fccnn.com/']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in available if x.find(\"cnn\") > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.thedailybeast.com/']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in available if x.find(\"dailybeast\") > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
