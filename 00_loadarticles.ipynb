{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions for how to build this using nbdev at https://nbdev.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load articles\n",
    "\n",
    "> Loads and holds news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import os, datetime, re\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load News Articles\n",
    "\n",
    "- Default position is that the news articles are in https://github.com/brockmanmatt/CoverageTrends\n",
    "- However, should add additional capabiltiies to pull different sets of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I can add new methods to my class by just inheriting and overwriting the old class essentially, cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder():\n",
    "    \"Basic unit to keep load and analze my articles\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.articleDir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "assert(test_ah.articleDir==None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder(article_holder):\n",
    "\n",
    "    def set_articleDir(self, path): self.articleDir = path\n",
    "    def get_articleDir(self): return self.articleDir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "test_ah.set_articleDir(path=\".\")\n",
    "assert(test_ah.get_articleDir()==\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Publication at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yahoonews',\n",
       " 'chicagotribune',\n",
       " 'nbcnews',\n",
       " 'foxnews',\n",
       " 'forbes',\n",
       " 'cnbc',\n",
       " 'sanfransiscochronicle',\n",
       " 'bostonglobe',\n",
       " '.DS_Store',\n",
       " 'newyorktimes',\n",
       " 'nydailynews',\n",
       " 'reuters',\n",
       " 'bbc',\n",
       " 'arstechnica',\n",
       " 'breitbart',\n",
       " 'washingtonpost',\n",
       " 'nypost',\n",
       " 'dailycaller',\n",
       " 'aljazeera',\n",
       " 'npr',\n",
       " 'rt',\n",
       " 'slate',\n",
       " 'sputnik',\n",
       " 'politico',\n",
       " 'cnn',\n",
       " 'buzzfeed',\n",
       " 'abcnews',\n",
       " 'livescience',\n",
       " 'techcrunch',\n",
       " 'dailybeast',\n",
       " 'newyorker',\n",
       " 'axios',\n",
       " 'nationalreview',\n",
       " 'businessinsider',\n",
       " 'theatlantic',\n",
       " 'fortune']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../CoverageTrends/archived_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CoverageTrendsLoader(publications:[str] = [], dateStart:str=None, dateEnd:str=None, lastN:int=None, verbose=False) -> []:\n",
    "    \n",
    "    \"\"\"\n",
    "    Turns CSV of scraped headlines from CoverageTrends into a Pandas Dataframe.\n",
    "    Expects that CoverageTrends (https://github.com/brockmanmatt/CoverageTrends) is cloned into ../CoverageTrends\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    publications: list of publications to try to pull from CoverageTrends CSV, all if []\n",
    "    \n",
    "    dateStart: String YYYYMMDD for first date of CSV to load for each publication\n",
    "    \n",
    "    dateEnd: String YYYYMMDD for last date of CSV to load for each publication\n",
    "    \n",
    "    lastN: get max (available days, lastN) days\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"Engine to load articles from CoverageTrends GitHub repo\"\n",
    "    if \"CoverageTrends\" not in os.listdir(\"..\"):\n",
    "        missingCoverageTrends=\"CoverageTrends engine requires CoverageTrends\"\n",
    "        missingCoverageTrends+=\"\\nPlease clone https://github.com/brockmanmatt/CoverageTrends to use this option\"\n",
    "        raise Exception(missingCoverageTrends)\n",
    "    \n",
    "    \"Make list of publications that have scraped lists\"\n",
    "    availablePublications = [x for x in os.listdir(\"../CoverageTrends/archived_links\") if x.find(\".\") ==-1]\n",
    "    \n",
    "    \"If publications are limited, then only go with those\"\n",
    "    if len(publications) > 0:\n",
    "        availablePublications = [x for x in publications if x in availablePublications]\n",
    "        \n",
    "    loaded_articles = []    \n",
    "    \n",
    "    \"Loop through each publisher in CoverageTrends and load each day\"\n",
    "    for publisher in availablePublications:\n",
    "        \n",
    "        csvPaths = []\n",
    "        \n",
    "        pubPath = \"{}/{}\".format(\"../CoverageTrends/archived_links\", publisher)\n",
    "        for month in os.listdir(pubPath):\n",
    "            if month.find(\".\") > -1:\n",
    "                continue\n",
    "            monthPath = \"{}/{}\".format(pubPath, month)\n",
    "            for day in os.listdir(monthPath):\n",
    "                if dateStart != None:\n",
    "                    if int(day.split(\"_\")[1][:-4]) < int(dateStart):\n",
    "                        continue\n",
    "                if dateEnd != None:\n",
    "                    if int(day.split(\"_\")[1][:-4]) > int(dateEnd):\n",
    "                        continue\n",
    "                csvPaths.append(\"{}/{}\".format(monthPath, day))\n",
    "        \n",
    "        csvPaths = sorted(csvPaths)\n",
    "        \n",
    "        if lastN != None:\n",
    "            csvPaths = csvPaths[-lastN:]\n",
    "        \n",
    "        csvPaths = pd.concat([pd.read_csv(x) for x in csvPaths], ignore_index=True)\n",
    "        csvPaths[\"source\"] = publisher\n",
    "        loaded_articles.append(csvPaths)\n",
    "\n",
    "    return pd.concat(loaded_articles).fillna(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(set([x[:8] for x in CoverageTrendsLoader(publications=[\"newyorktimes\"], lastN=3).date.unique()]))==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder(article_holder):\n",
    "    \"gives article_holder ability to load articles for publication between dateStart and dateEnd\"\n",
    "    \n",
    "    def load_articles(self, engine=CoverageTrendsLoader, publications:[str] = [], dateStart:str=None, dateEnd:str=None, lastN:int=None, verbose=False) -> []:\n",
    "        if self.articleDir == None:\n",
    "            raise Exception(\"holder missing path\")\n",
    "\n",
    "        self.df = engine(publications=publications, dateStart=dateStart, dateEnd=dateEnd, lastN=lastN, verbose=verbose)\n",
    "        \n",
    "        tmp = pd.DataFrame([self.df.text.unique()]).T\n",
    "        tmp.columns=[\"text\"]\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        tmp[\"quickReplace\"] = tmp[\"text\"].fillna(\"\").apply(lambda x: re.sub('[^a-z]+', \" \", x.lower()))\n",
    "        tmp[\"tokens\"] = tmp[\"quickReplace\"].apply(lambda x: [stemmer.stem(y) for y in x.split() if len (y) > 0])\n",
    "        tmp[\"quickReplace\"] = tmp[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "        self.df = self.df.merge(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "try:\n",
    "    test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "    assert False\n",
    "except:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah.set_articleDir(\".\")\n",
    "try:\n",
    "    test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "    assert True\n",
    "except:\n",
    "    assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
