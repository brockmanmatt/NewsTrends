{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions for how to build this using nbdev at https://nbdev.fast.ai/\n",
    "\n",
    "#I should throw this into pip and then can import it into CoverageTrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load articles\n",
    "\n",
    "> Loads and holds news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import os, datetime, re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load News Articles\n",
    "\n",
    "- Default position is that the news articles are in https://github.com/brockmanmatt/CoverageTrends\n",
    "- However, should add additional capabiltiies to pull different sets of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I can add new methods to my class by just inheriting and overwriting the old class essentially, cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder():\n",
    "    \"Basic unit to keep load and analze my articles\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.articleDir = None\n",
    "        self.df = None\n",
    "        \n",
    "        try:\n",
    "            stopwords = text.ENGLISH_STOP_WORDS\n",
    "        except:\n",
    "            print(\"stopwords not found, downloading\")\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "    extra_stopwords = [\"news\", \"say\", \"said\", \"told\", \"tell\", \"day\", \"video\", \"week\", \"state\", \"new\", \"york\", \"times\"]\n",
    "    stopwords = text.ENGLISH_STOP_WORDS.union(extra_stopwords)\n",
    "\n",
    "    subclass=\"article_holder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "assert(test_ah.articleDir==None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder(article_holder):\n",
    "\n",
    "    def set_articleDir(self, path): self.articleDir = path\n",
    "    def get_articleDir(self): return self.articleDir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "test_ah.set_articleDir(path=\"../CoverageTrends\")\n",
    "assert(test_ah.get_articleDir()==\"../CoverageTrends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Publication at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yahoonews',\n",
       " 'chicagotribune',\n",
       " 'nbcnews',\n",
       " 'foxnews',\n",
       " 'forbes',\n",
       " 'cnbc',\n",
       " 'sanfransiscochronicle',\n",
       " 'bostonglobe',\n",
       " '.DS_Store',\n",
       " 'newyorktimes',\n",
       " 'nydailynews',\n",
       " 'reuters',\n",
       " 'bbc',\n",
       " 'arstechnica',\n",
       " 'breitbart',\n",
       " 'washingtonpost',\n",
       " 'nypost',\n",
       " 'dailycaller',\n",
       " 'aljazeera',\n",
       " 'npr',\n",
       " 'rt',\n",
       " 'slate',\n",
       " 'sputnik',\n",
       " 'politico',\n",
       " 'cnn',\n",
       " 'buzzfeed',\n",
       " 'abcnews',\n",
       " 'livescience',\n",
       " 'techcrunch',\n",
       " 'dailybeast',\n",
       " 'newyorker',\n",
       " 'axios',\n",
       " 'nationalreview',\n",
       " 'businessinsider',\n",
       " 'theatlantic',\n",
       " 'fortune']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../CoverageTrends/archived_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CoverageTrendsLoader(publications:[str] = [], path=\".\", dateStart:str=None, dateEnd:str=None, lastN:int=None, verbose=False, **kwargs) -> []:\n",
    "    \n",
    "    \"\"\"\n",
    "    Turns CSV of scraped headlines from CoverageTrends into a Pandas Dataframe.\n",
    "    Expects that CoverageTrends (https://github.com/brockmanmatt/CoverageTrends) is cloned into ../CoverageTrends\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    publications: list of publications to try to pull from CoverageTrends CSV, all if []\n",
    "    \n",
    "    dateStart: String YYYYMMDD for first date of CSV to load for each publication\n",
    "    \n",
    "    dateEnd: String YYYYMMDD for last date of CSV to load for each publication\n",
    "    \n",
    "    lastN: get max (available days, lastN) days\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"Engine to load articles from CoverageTrends GitHub repo\"\n",
    "    if \"archived_links\" not in os.listdir(path):\n",
    "        missingCoverageTrends=\"CoverageTrends engine requires CoverageTrends\"\n",
    "        missingCoverageTrends+=\"\\nPlease clone https://github.com/brockmanmatt/CoverageTrends to use this option\"\n",
    "        raise Exception(missingCoverageTrends)\n",
    "    \n",
    "    \"Make list of publications that have scraped lists\"\n",
    "    availablePublications = [x for x in os.listdir(\"{}/archived_links\".format(path)) if x.find(\".\") ==-1]\n",
    "    \n",
    "    \"If publications are limited, then only go with those\"\n",
    "    if len(publications) > 0:\n",
    "        availablePublications = [x for x in publications if x in availablePublications]\n",
    "        \n",
    "    loaded_articles = []    \n",
    "    \n",
    "    \"Loop through each publisher in CoverageTrends and load each day\"\n",
    "    for publisher in availablePublications:\n",
    "        \n",
    "        csvPaths = []\n",
    "        \n",
    "        pubPath = \"{}/{}\".format(\"{}/archived_links\".format(path), publisher)\n",
    "        for month in os.listdir(pubPath):\n",
    "            if month.find(\".\") > -1:\n",
    "                continue\n",
    "            monthPath = \"{}/{}\".format(pubPath, month)\n",
    "            for day in os.listdir(monthPath):\n",
    "                if dateStart != None:\n",
    "                    if int(day.split(\"_\")[1][:-4]) < int(dateStart):\n",
    "                        continue\n",
    "                if dateEnd != None:\n",
    "                    if int(day.split(\"_\")[1][:-4]) > int(dateEnd):\n",
    "                        continue\n",
    "                csvPaths.append(\"{}/{}\".format(monthPath, day))\n",
    "        \n",
    "        csvPaths = sorted(csvPaths)\n",
    "        \n",
    "        if lastN != None:\n",
    "            csvPaths = csvPaths[-lastN:]\n",
    "        \n",
    "        csvPaths = pd.concat([pd.read_csv(x) for x in csvPaths], ignore_index=True)\n",
    "        csvPaths[\"source\"] = publisher\n",
    "        loaded_articles.append(csvPaths)\n",
    "\n",
    "    return pd.concat(loaded_articles).fillna(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(set([x[:8] for x in CoverageTrendsLoader(path=\"../CoverageTrends\", publications=[\"newyorktimes\"], lastN=3).date.unique()]))==3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dailysourcepermalinksLoader(path=\"\", filename=\"\", **kwargs) -> pd.DataFrame:\n",
    "    df = pd.read_json(\"{}/{}\".format(path, filename)).T\n",
    "    df[\"date\"] = df.index\n",
    "    df = df.melt(id_vars = [\"date\"], var_name=\"Source\", value_name=\"text\") #make big column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder(article_holder):\n",
    "    \"gives article_holder ability to load articles for publication between dateStart and dateEnd\"\n",
    "    \n",
    "    def load_articles(self, engine=CoverageTrendsLoader, publications:[str] = [], dateStart:str=None, dateEnd:str=None, lastN:int=None, verbose=False, **kwargs) -> []:\n",
    "        if self.articleDir == None:\n",
    "            raise Exception(\"holder missing path\")\n",
    "\n",
    "        self.df = engine(path=self.articleDir, publications=publications, dateStart=dateStart, dateEnd=dateEnd, lastN=lastN, verbose=verbose, **kwargs)\n",
    "        \n",
    "        tmp = pd.DataFrame([self.df.text.unique()]).T\n",
    "        tmp.columns=[\"text\"]\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        tmp[\"quickReplace\"] = tmp[\"text\"].fillna(\"\").apply(lambda x: re.sub('[^a-z]+', \" \", x.lower()))\n",
    "        tmp[\"tokens\"] = tmp[\"quickReplace\"].apply(lambda x: [stemmer.stem(y) for y in x.split() if len (y) > 0])\n",
    "        tmp[\"quickReplace\"] = tmp[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "        self.df = self.df.merge(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "try:\n",
    "    test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "    assert False\n",
    "except:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "test_ah.set_articleDir(path=\"../CoverageTrends\")\n",
    "try:\n",
    "    test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "    assert (\"quickReplace\" in test_ah.df.columns)\n",
    "except:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\r\n",
      "dailysourcepermalinks.onemonthsample.json\r\n"
     ]
    }
   ],
   "source": [
    "ls ../testNewstrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ah = article_holder()\n",
    "# test_ah.set_articleDir(path=\"../testNewstrends\")\n",
    "# test_ah.load_articles(engine=dailysourcepermalinksLoader, filename=\"dailysourcepermalinks.onemonthsample.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I want to be able to load dfs from other holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class article_holder(article_holder):\n",
    "    \"article_holders should be able to load dfs from other article_holders\"\n",
    "\n",
    "    def load_article_holder(self, other_article_holder:article_holder):\n",
    "        errorMessage = \"Bad format\"\n",
    "\n",
    "        try:\n",
    "            missing = []\n",
    "            otherColumns = other_article_holder.df.columns\n",
    "            if \"quickReplace\" not in otherColumns:\n",
    "                missing.append(\"quickReplace\")\n",
    "            if \"tokens\" not in otherColumns:\n",
    "                missing.append(\"tokens\")\n",
    "            if \"text\" not in otherColumns:\n",
    "                missing.append(\"text\")\n",
    "            if \"source\" not in otherColumns:\n",
    "                missing.append(\"source\")\n",
    "            if len(missing) > 0:\n",
    "                errorMessage = \"Missing {}\".format(\";\".join(missing))\n",
    "                raise Exception(\"error\")\n",
    "            \n",
    "        except:\n",
    "            raise Exception(\"Passed article holder structure not recognized: {}\".format(errorMessage))\n",
    "        \n",
    "        self.df = other_article_holder.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "try:\n",
    "    test_ah.load_article_holder(test_ah)\n",
    "    assert False\n",
    "except:\n",
    "    assert True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = article_holder()\n",
    "test_ah.set_articleDir(path=\"../CoverageTrends\")\n",
    "test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "\n",
    "test_ah2 = article_holder()\n",
    "\n",
    "test_ah2.load_article_holder(test_ah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
