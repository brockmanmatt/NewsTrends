{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions for how to build this using nbdev at https://nbdev.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict loaded articles\n",
    "\n",
    "> Takes a loader as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from newstrends import loader, describe\n",
    "import pandas as pd\n",
    "import datetime, os\n",
    "from datetime import timezone\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from pmdarima import auto_arima\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class visualizer:\n",
    "    \"my visualizer! Don't know what I'm doing with it yet though\"\n",
    "    \n",
    "    def __init__(self, coverageTrendsPath=\".\", workdir = \"docs\"):\n",
    "        \"\"\" initializing building a list of all the pkls in workdir\"\"\"\n",
    "        self.coverageTrendsPath = coverageTrendsPath\n",
    "        self.workdir = workdir\n",
    "        self.outfir = outdir\n",
    "        \n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "        self.colors = [\"orange\", \"green\", \"red\", \"brown\", \"blue\", \"yellow\", \"pink\"]\n",
    "    \n",
    "    \n",
    "    def runDefault(self):\n",
    "        \n",
    "        outdir=self.workdir\n",
    "        \n",
    "        publisherList = [\"newyorktimes\", \"washingtonpost\"]\n",
    "        \n",
    "        describer = describe.describer()\n",
    "        \n",
    "        describer.set_articleDir(path=self.coverageTrendsPath)\n",
    "        describer.load_articles(publications=publisherList, lastN=5)\n",
    "        describer.fitVectorizer(ngram_range=(1,1))\n",
    "        \n",
    "        topN = pd.DataFrame()\n",
    "        \n",
    "        for publisher in publisherList:\n",
    "            topN[publisher] = [x[0] for x in describer.getTopNWords(10, source=[publisher])]\n",
    "            \n",
    "        vcs = topN.melt(var_name='publisher', value_name='words')[\"words\"].value_counts()\n",
    "        myTime = datetime.datetime.now(tz=timezone.utc).strftime('%Y%m%d-%H%M')\n",
    "        myTime = myTime[:-1]\n",
    "        myTime +=\"0\"\n",
    "        plt.close('all') #in case of zombies or something\n",
    "        os.makedirs(\"{}/img\".format(outdir), exist_ok=True)\n",
    "        os.makedirs(\"{}/timeseries\".format(outdir), exist_ok=True)\n",
    "\n",
    "        df = describer.df\n",
    "                \n",
    "        \"\"\"\n",
    "        If I remember what this is doing, I'm using counter to find signifcant terms, then going and grpahing\n",
    "        them, saving the time series in a pkl in outdir;\n",
    "        \n",
    "        The thing is, the way I'm building this, I want to use pip to import to the CoverageTrends folder, so\n",
    "        path should be \".\" and not \"../CoverageTrends as I'm doing here; so what I want is visualzier to have\n",
    "        a path to coverage_trends!\"\n",
    "        \"\"\"\n",
    "        for middleWord in vcs.dropna().index: #k, this is going to be wayyy too many images, but just testing\n",
    "\n",
    "            tmp = df[df[\"tokens\"].apply(lambda x: (middleWord in x))].copy().fillna(0)\n",
    "            tmp.date = pd.to_datetime(tmp.date)\n",
    "            tmp = tmp.groupby([\"source\", \"date\"]).count()[\"quickReplace\"]\n",
    "            try:  #for some reason, sometimes the formatting's getting messed up\n",
    "                tmp.unstack(level=0).fillna(0).to_pickle(\"{}/timeseries/{}.pkl\".format(outdir, middleWord))\n",
    "            except:\n",
    "                pass\n",
    "            ax = tmp.unstack(level=0).fillna(0).plot(title=\"Frontpage mentions of {}\".format(middleWord), figsize=(8,8))\n",
    "            ax.set_ylabel(\"frontpage mentions at time\")\n",
    "            try:\n",
    "                deleteMe = [oldFile for oldFile in os.listdir(\"{}/img\".format(outdir)) if oldFile.endswith(middleWord+\".jpg\")]\n",
    "                for oldFile in deleteMe:\n",
    "                    os.remove(\"{}/img/{}\".format(outdir, oldFile))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            ax.figure.savefig(\"{}/img/{}_{}.jpg\".format(outdir, myTime, middleWord))\n",
    "            plt.close('all') #close all figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class visualizer(visualizer):\n",
    "    \" build models for time series as done from CoverageTrends \"\n",
    "\n",
    "    def buildModels(self, verbose=False):\n",
    "        \n",
    "        workdir = self.workdir\n",
    "        \n",
    "        self.targetPaths = []\n",
    "        \n",
    "        for filename in os.listdir(\"{}/timeseries\".format(workdir)):\n",
    "            if filename.endswith(\".pkl\"):\n",
    "                self.targetPaths.append(\"{}/timeseries/{}\".format(workdir, filename))\n",
    "\n",
    "        if verbose:\n",
    "            print(self.targetPaths)\n",
    "        \"\"\" build a model for each target in targetPaths using fnc \"\"\"\n",
    "        for target in self.targetPaths:\n",
    "            print(target)\n",
    "            try:\n",
    "                df = pd.read_pickle(target)\n",
    "                #self.buildQuickVAR(df, target.split(\"/\")[-1][:-4])\n",
    "                if verbose:\n",
    "                    print(\"var done\")\n",
    "                myFreq = \"3h\"\n",
    "                self.buildQuickSARIMAX(df.resample(myFreq).mean().fillna(0), target.split(\"/\")[-1][:-4], freq=8)\n",
    "                #os.remove(\"{}\".format(target))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def buildQuickVAR(self, df, name, test_size=-1, validation_size=-1):\n",
    "        \"\"\" builds VAR model for series \"\"\"\n",
    "        os.makedirs(\"{}/VAR\".format(self.workdir), exist_ok=True)\n",
    "\n",
    "        #Fit model\n",
    "        model = VAR(df)\n",
    "        results = model.fit(maxlags=24, ic='aic')\n",
    "\n",
    "        #Get forecast\n",
    "        lag_order = results.k_ar\n",
    "        newVals = pd.DataFrame(results.forecast(df.values[-lag_order:], 24))\n",
    "        newVals.index = [df.index.max()+datetime.timedelta(minutes=30*x) for x in range(1,25)]\n",
    "        newVals.columns = df.columns\n",
    "        newVals = results.fittedvalues.append(newVals)\n",
    "\n",
    "        #plot\n",
    "        ax = newVals.plot(style=\":\", figsize=(8,8), color=self.colors, title=\"VAR Quick Fit for {}\".format(name))\n",
    "        df.plot(ax=ax, color=self.colors, legend=False)\n",
    "\n",
    "        ax.figure.savefig(\"{}/VAR/{}.jpg\".format(self.workdir, name))\n",
    "        plt.close('all') #close all figures\n",
    "\n",
    "    def buildQuickSARIMAX(self, df, name, freq=24, test_size=-1, validation_size=-1):\n",
    "        \"\"\" takes dataframe of time series and builds a SARIMAX model for each column \"\"\"\n",
    "        \"\"\" seasonality is daily for now, which is 48 time step thingies\"\"\"\n",
    "        \"\"\" for now, just fiting to training data; will truncate last day starting next week for testing as well\"\"\"\n",
    "\n",
    "        os.makedirs(\"{}/SARIMAX\".format(self.workdir), exist_ok=True)\n",
    "\n",
    "        \"\"\"\n",
    "        k, doing this at the 30 minute aggregate is WAY too slow, so resampling hour\n",
    "        \"\"\"\n",
    "\n",
    "        corr_df = df.copy()\n",
    "        for i in range(1,13):\n",
    "            corr_df = pd.concat([corr_df, df.diff(i).add_prefix(\"L{}_\".format(i))], axis=1)\n",
    "\n",
    "        corr_df = np.abs(corr_df)\n",
    "        corr_df = corr_df.dropna().corr()[df.columns][len(df.columns):]\n",
    "\n",
    "        #since I generated this, I might as well save it to a csv (js can't read pkl)\n",
    "        os.makedirs(\"{}/{}\".format(self.workdir, \"corr\"), exist_ok=True)\n",
    "        corr_df.to_csv(\"{}/{}/{}.csv\".format(self.workdir, \"corr\", name))\n",
    "\n",
    "        max_lag = -1\n",
    "        results_df = df.copy()\n",
    "        \n",
    "        print(name)\n",
    "\n",
    "        #plot SARIMAX using best correlating lag of an exogenous\n",
    "        #does some magic to do a bunch of forecasts\n",
    "        #I probably should be measuring errors =/\n",
    "        for column in df.columns:\n",
    "            #get max lag - we'll plot them all together so the maxlag is going to be the max_lag\n",
    "            best_series = corr_df[corr_df.index.map(lambda x: not x.endswith(column))][column].idxmax()\n",
    "            lag = int(best_series.split(\"_\")[0][1:])\n",
    "            if lag > max_lag:\n",
    "                max_lag = lag\n",
    "\n",
    "            exog = best_series.split(\"_\")[1]\n",
    "\n",
    "            endogenous = df[[column]][lag:].copy()\n",
    "            exogenous = df[[exog]].shift(lag)[lag:]\n",
    "            model = auto_arima(endogenous, exogenous=exogenous, scoring=\"mae\", out_of_sample_size=freq, m=freq, stepwise=True)\n",
    "            endogenous[column]=model.predict_in_sample(exogenous=exogenous)\n",
    "            forecasts = pd.DataFrame()\n",
    "            forecasts[column] = model.predict(n_periods=lag, exogenous=df[[exog]][-lag:])\n",
    "            forecasts.index = [endogenous.index[-1] + x*endogenous.index[-1].freq for x in range(1,lag+1)]\n",
    "            endogenous = endogenous.append(forecasts)\n",
    "            while results_df.index.max() < endogenous.index.max():\n",
    "                results_df = results_df.append(pd.Series(name=results_df.index[-1] + results_df.index[-1].freq))\n",
    "            results_df[column] = endogenous[column]\n",
    "\n",
    "        ax = results_df[max_lag:].plot(legend=False, style=\":\", color=self.colors, title=name, figsize=(8,8))\n",
    "        df.plot(ax=ax, style=\"-\", color=self.colors, legend=True)\n",
    "\n",
    "        ax.figure.savefig(\"{}/SARIMAX/{}.jpg\".format(self.workdir, name))\n",
    "        plt.close('all') #close all figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = visualizer(coverageTrendsPath=\"../CoverageTrends\", workdir=\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.runDefault()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
