{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instructions for how to build this using nbdev at https://nbdev.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe loaded articles\n",
    "\n",
    "> Takes a loader as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from newstrends import loader\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class describer(loader.article_holder):\n",
    "    \"inherit everything from article_holder including init\"\n",
    "    \n",
    "    subclass=\"describer\"\n",
    "    vectorizer=None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = loader.article_holder()\n",
    "assert(type(tmp)==loader.article_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'article_holder'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'describer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = describer()\n",
    "tmp.subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure that I can still use article_holder functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = describer()\n",
    "test_ah.set_articleDir(\"../CoverageTrends\")\n",
    "try:\n",
    "    test_ah.load_articles(publications=[\"newyorktimes\"])\n",
    "    assert (\"quickReplace\" in test_ah.df.columns)\n",
    "except:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class describer(describer):\n",
    "    \"Add in vectorize\"\n",
    "    \n",
    "    def fitVectorizer(self, vectorizer:CountVectorizer=CountVectorizer, ngram_range=(1,2), max_features=10000):\n",
    "        try:\n",
    "            _ = self.df[:1].quickReplace\n",
    "        except:\n",
    "            raise Exception(\"No article data found\")\n",
    "        \n",
    "        self.vectorizer=vectorizer(stop_words=self.stopwords, ngram_range=ngram_range, max_features=max_features).fit(self.df.quickReplace)        \n",
    "    \n",
    "    def getTopNWords(self, topN = 10, lastDate=None, window=None, source=None):        \n",
    "        \" get topN important words for each publication \"\n",
    "\n",
    "        \"check if properly formatted\"\n",
    "        if type(self.df) != pd.core.frame.DataFrame:\n",
    "            raise Exception(\"Dataframe not loaded\")\n",
    "        if self.vectorizer == None:\n",
    "            raise Exception(\"No vectorizer found\")\n",
    "        \n",
    "        \n",
    "        \"Get Dataframe for source and time period\"\n",
    "        sources=source\n",
    "        if source==None:\n",
    "            sources=[x for x in self.df.source.unique()]\n",
    "        df = self.df[self.df.source.isin(sources)]\n",
    "        \n",
    "        \"get counts of features from count vectorizer\"\n",
    "        X = self.vectorizer.transform(df.quickReplace)\n",
    "        vocab = list(self.vectorizer.get_feature_names())\n",
    "        counts = X.sum(axis=0).A1\n",
    "        counts = Counter(dict(zip(vocab, counts)))\n",
    "\n",
    "        return counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure exceptions thrown on empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ah = describer()\n",
    "try:\n",
    "    test_ah.fitVectorizer()\n",
    "    assert False\n",
    "except:\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try loading in new york times data to test vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = describer()\n",
    "test.set_articleDir(path=\"../CoverageTrends\")\n",
    "test.load_articles(publications=[\"newyorktimes\"])\n",
    "test.fitVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 3378),\n",
       " ('time', 2846),\n",
       " ('citi', 2801),\n",
       " ('protest', 2312),\n",
       " ('coronavirus', 1773),\n",
       " ('pandem', 1610),\n",
       " ('presid', 1589),\n",
       " ('press', 1458),\n",
       " ('polic', 1428),\n",
       " ('death', 1403)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.getTopNWords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class describer(describer):\n",
    "    \"Now having cooccurances could be nice\"\n",
    "    \n",
    "    def generateCoOccurances(self, pubList = [\"newyorktimes\", \"foxnews\", \"washingtonpost\", \"cnn\", \"breitbart\", \"abcnews\", \"dailycaller\"], verbose=False, topK:\"int<100\" = 20):\n",
    "        \" get cooccurances of terms in my df, up to 100\"\n",
    "        \n",
    "        if type(self.df) != pd.core.frame.DataFrame:\n",
    "            raise Exception(\"Dataframe not loaded\")\n",
    "        if self.vectorizer == None:\n",
    "            raise Exception(\"No vectorizer found\")\n",
    "            \n",
    "        vectorizer = CountVectorizer(stop_words=self.stopwords, max_features=10000).fit(self.df.quickReplace)   \n",
    "\n",
    "        # get the transformed DF\n",
    "        X = vectorizer.transform(self.df.quickReplace)\n",
    "        X[X > 0] = 1\n",
    "\n",
    "        coOccurance = (X.T * X)\n",
    "        coOccurance.setdiag(0)\n",
    "        d = coOccurance.todense()\n",
    "        \n",
    "        checkLength = topK*2\n",
    "        if checkLength > 100:\n",
    "            checkLength = 100\n",
    "\n",
    "        top_prs = np.dstack(np.unravel_index(np.argpartition(d.ravel(),-checkLength)[:,-checkLength:],d.shape))[0]\n",
    "\n",
    "        vals = []\n",
    "        keys = vectorizer.get_feature_names()\n",
    "        for pair in top_prs:\n",
    "            newEntry = [keys[pair[0]], keys[pair[1]]]\n",
    "            if newEntry not in vals:\n",
    "                vals.append(newEntry)\n",
    "            if len(vals) >= topK:\n",
    "                break\n",
    "\n",
    "        #So now for each day for each time period I want to math out the co-occurances!\n",
    "        return vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k, so some work to do here on making this work with this framework.\n",
    "\n",
    "def getRecentInterestingGroups(self, pubList = [\"newyorktimes\", \"foxnews\", \"washingtonpost\", \"cnn\", \"breitbart\", \"abcnews\", \"dailycaller\"], outdir = \"docs\"):\n",
    "        vals =self.generateCoOccurances(dateStart=(datetime.datetime.today()-datetime.timedelta(days=1)).strftime(\"%Y%m%d\"), )\n",
    "\n",
    "        grps = {}\n",
    "        idx = 0\n",
    "        for val in vals:\n",
    "            found = False\n",
    "            for grp in grps:\n",
    "                if val[0] in grps[grp]:\n",
    "                    grps[grp].add(val[1])\n",
    "                    found=True\n",
    "                    continue\n",
    "                elif val[1] in grps[grp]:\n",
    "                    grps[grp].add(val[0])\n",
    "                    found=True\n",
    "                    continue\n",
    "\n",
    "            if not found:\n",
    "                grps[idx] = set()\n",
    "                grps[idx].add(val[0])\n",
    "                grps[idx].add(val[1])\n",
    "                idx +=1\n",
    "\n",
    "        myTargets = [x[1] for x in grps.items() if len(x[1]) < 4]\n",
    "        print(\"targets: {}\".format(myTargets))\n",
    "\n",
    "        self.loadArticles(pubList=pubList)\n",
    "        print(\"building bigdf2\")\n",
    "        self.buildBigDF()\n",
    "\n",
    "        myTime = datetime.datetime.now(tz=timezone.utc).strftime('%Y%m%d-%H%M')\n",
    "        myTime = myTime[:-1]\n",
    "        myTime +=\"0\"\n",
    "\n",
    "        plt.close('all') #in case of zombies or something\n",
    "        os.makedirs(\"{}/img\".format(outdir), exist_ok=True)\n",
    "        os.makedirs(\"{}/timeseries\".format(outdir), exist_ok=True)\n",
    "\n",
    "        for target_words in myTargets:\n",
    "            print(\"making df for {}\".format(target_words))\n",
    "            tmp = self.bigdf[self.bigdf.tokens.apply(lambda x: len(set(x))==len(target_words|set(x)))]\n",
    "\n",
    "            print(len(tmp))\n",
    "\n",
    "            tmp.date = pd.to_datetime(tmp.date)\n",
    "            tmp = tmp.groupby([\"source\", \"date\"]).count()[\"quickReplace\"]\n",
    "\n",
    "            print(\"making source series for {}\".format(target_words))\n",
    "            tmp.unstack(level=0).fillna(0).to_pickle(\"{}/timeseries/{}.pkl\".format(outdir, \"+\".join(target_words)))\n",
    "\n",
    "            print(\"making plot for {}\".format(target_words))\n",
    "            ax = tmp.unstack(level=0).fillna(0).plot(title=\"Frontpage mentions of {}\".format(\"+\".join(target_words)), figsize=(8,8))\n",
    "            ax.set_ylabel(\"frontpage mentions at time\")\n",
    "\n",
    "            deleteMe = [oldFile for oldFile in os.listdir(\"{}/img\".format(outdir)) if oldFile.endswith(\"+\".join(target_words)+\".jpg\")]\n",
    "            for oldFile in deleteMe:\n",
    "                os.remove(\"docs/img/{}\".format(oldFile))\n",
    "\n",
    "            ax.figure.savefig(\"{}/img/{}_{}.jpg\".format(outdir, myTime, \"+\".join(target_words)))\n",
    "            plt.close('all') #close all figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = describer()\n",
    "test.set_articleDir(path=\".\")\n",
    "test.load_articles(publications=[\"newyorktimes\"])\n",
    "test.fitVectorizer()\n",
    "\n",
    "assert(len(test.generateCoOccurances(topK=15))==15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
